# Songbird: A Deep Learning Approach for Jazz/Blues Generation

## Overview

**Songbird** is a student project from the Spring 2025 Deep Learning course. It explores deep learning-based symbolic music generation, focusing on expressive jazz and blues using autoregressive Transformer models. The model takes MIDI-based note sequences and produces stylistically rich, polyphonic jazz compositions. We also incorporated reinforcement learning (REINFORCE) to improve musicality.

## Key Features

* Autoregressive Transformer-based architecture
* Reinforcement Learning (REINFORCE) for musical refinement
* Polyphonic output with support for chords, rests, rhythm
* PyTorch + TensorFlow dual implementations for experimentation
* Music tokenization pipeline (pitch + duration)

## Project Architecture

The model generates music in an autoregressive way, one token at a time. Each token encodes both pitch and rhythmic duration (e.g., `C4_0.25`). After supervised training, Songbird is further optimized using Monte Carlo REINFORCE to maximize custom musical reward functions:

* Harmonic Consistency
* Melodic Contour
* Rhythm Diversity
* Interval Quality

## Dataset & Preprocessing

* **Source**: Jazz MIDI collections including "Jazz ML ready MIDI" and curated files (e.g., Doug McKenzie's piano transcriptions)
* **Token Format**: `{pitch}_{duration}` (e.g., `C4_1.0`, `C4.E4.G4_0.5`)
* **Cleaning**: Filtered out unsuitable tracks, removed invalid durations, kept single-instrument melodies
* **Training Data**: Converted to token sequences of length 100 for input-output prediction pairs

## Evaluation Metrics

* **Training/Testing Loss**: Cross-entropy loss on next-token prediction
* **Human Feedback**: Early-stage subjective evaluation for musicality
* **Reward Functions (RL phase)**:

  * Interval quality score
  * Harmonic consistency
  * Rhythm diversity
  * Melodic contour score

## File Structure

```
songbird-main/
├── jazz_and_stuff.zip              # Collection of jazz MIDI files
├── rl_best.h5                      # Saved weights after RL fine-tuning
├── iters/                          # MIDI files generated by the model
│   └── *.mid
├── src/
│   ├── main.ipynb                  # Summary of experiments
│   └── armstrong-build/
│       ├── arm.sh / gen.sh         # Shell scripts for training and generation
│       ├── train_armstrong.py      # Supervised training code
│       ├── gen_armstrong.py        # MIDI generation code
│       ├── monk_lambda.py          # TensorFlow + RL Transformer (monk)
│       ├── monk_torch.py           # PyTorch version of Transformer
│       └── jazz_and_stuff/         # Expanded MIDI dataset
```

## How to Run

You can run the project using any Ubuntu terminal.

1. Make sure `jazz_and_stuff.zip` is unzipped in the correct directory.
2. Run the model with supervised or RL-based Transformer by modifying the script:

```bash
bash monk_docker.sh
```

3. To switch between **supervised** and **reinforcement learning** versions:

* Edit `monk_docker.sh` to call either `monk_lambda.py` or `monk_lambda_new.py`

  * `monk_lambda.py` = baseline Transformer (no RL)
  * `monk_lambda_new.py` = RL fine-tuned Transformer (with REINFORCE)

4. To run training and generation separately:

```bash
bash arm.sh         # Train the model
bash gen.sh         # Generate jazz samples
```

Contributions

| Name          | Task                                                                                                                                                     | File Names                                                                                 | No. Lines of Code |
| ------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ----------------- |
| Sadman Kabir  | - Implemented Songbird_LSTM. <br> - Implemented the first iteration of the Transformer Model. <br> - Added full Tokenization support for chords, rests, and varying note duration combinations. <br> - Implemented REINFORCE for RL fine-tuning. | - armstrong_tensorflow.ipynb <br> - train_armstrong.py <br> - gen_armstrong.py <br> - .sh scripts for the above <br> - monk_lambda_new_.py <br> - monk.py <br> - monk.sh | ~800              |
| Chen Han Lin  | - Developed first iteration of Songbird_Transformer <br> - Developed preprocessing system for dataset <br> - Curated dataset files to expand size <br> - Trained Transformer models | - monk_torch.py <br> - monk_torch.sh <br> - Trained all torch-based models <br> - Generated outputs for torch models | ~600              |
| Xushuai Zhang | - Trained almost every model <br> - Contributed heavily to the RL-based Transformer model named *monk* <br> - Wrote PyTorch versions of Transformer for parallel training | - monk_lambda.py <br> - monk_torch_new_.py <br> - Related .sh file <br> - Trained nearly every model iteration | ~500              |


## GitHub

[https://github.com/corndog-overflow/songbird](https://github.com/corndog-overflow/songbird)

## Dependencies

* Python 3.x
* PyTorch or TensorFlow
* music21
* pretty\_midi
* numpy, argparse

## License

For academic use only. Developed as part of BU’s Spring 2025 Deep Learning course.
