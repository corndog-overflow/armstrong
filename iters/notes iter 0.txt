https://bushgrafts.com/midi/


trained on 200 epochs, changing to 250
using rms prop, changing to adam
no second relu, changed network to have 2 relus
dropout .2, changing to .5
no rests, shorter vocabulary
added: 


originally: 20 iput seq

100 length sequence -> generates 1 next predicted token -> 101 - 101[0] 


AABBCC -> D -> ABBCCD -> generate -> D -> BBCCDD -> loop 


another thing i changed: old use was: argmax 




[....], argmax to get highest probability

something that happens is that we get long loops of the same note. good music AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA good music

good music = not earbleedingly bad. 


have a solution:

[................................................................................] = size of vocab, of all notes, prob dist. adds to 1. 
argmax = picks highest one. next most likely token. 


AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA


get 10 highest prob notes. F#, A, B ,C top 10


normalize top 5, <- will have sum probability 1


.33, .15 ... = sum to 1

.33 note will be picked 1/3 of the time. 




cant just do music gen. 


ideas: 1. train it on jazz music.
todo: get more jazz.


idea 2: RL stuff. 

an idea we had was to use frechet audio distance

we cant do that
nah


self referential metrics (dont need to compare to another song)


if :


A A A A A A 

negative RL reward, add to loss. 



another idea:

GAN


another model

classifies midi as either jazz or not jazz

gen model make songs, put it into GAN classifier, if gan thinks its not jazz, up the loss on the gen model.


^ a lot of work.


think abt these:

how do we judge music based on the music alone

get more data




MUST DOS:

learn scc

somebody: turn the lstm w tf keras into a transformer with torch
whoever does this, AI it works it works


RL metric: some music evaluation metric note: this  should be self-referential meaning 

AAAAAAA dock the loss


hyperparameters must change:

already changed optimizer
already added a relu, might have to delete
architecture itself, batch norm, more layers. play with thing.


sequence length < should play with.

what might be fun


branches

hanks
me
alex


*whenever training use A100
*A100 takes 2 hours + to train



for presentations: would be great to have both an lstm and a transformer to compare. 
i can make presentation



AT THE END OF THE DAY:

goal: good sounding music. 



something to look out for: prev implementations plateau at .3 loss.
MAYBE high 2s. 



whyu more than 1 instrument is bad:

1. dont have time, gpu. 
2. vocab WAAAAY BIGGER, means harder to train
3. _ _ _  _  _ _ drums for example
4. sax weird mix of notes


what ill send: a python script that checks if something has more than 1 instrument or not
also: if you like a song, that is blues or jazzy, or maybe not even jazz, find a midi, add it to the data

also look at the scc



todo: im gonna add durations to vocab


to do: add more vocab points for note durations (much larger vocabulary)




todo: parallelize

convert lstm tf keras into transformer, torch

curate dataset

add updates: adam. 